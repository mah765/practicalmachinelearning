---
title: 'Practical ML in R: Final Project'
author: "MH"
date: "11/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Background
This is the final project for the Coursera Machine Learning in R course.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Load necessary packages + data
First we load a few packages.
```{r packages, echo=TRUE}
suppressMessages(library(tidyverse))
suppressMessages(library(readr))
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(gbm))
suppressMessages(library(reshape2))
```

Next we load the training and test sets.
```{r load data, echo=FALSE, message=FALSE, warning=FALSE}
train_raw <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
test <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

## Explore data structure

Here we check the data structure, and then look for ways to reduce data complexity and prep it for the model fitting. Generally it seems we are able to reduce the number of predictors using PCA, as well as remove a few variables that do not add any information that is helpful for classification modeling.

```{r structure, echo=TRUE}
# check out the structure
dim(train_raw)
# let's remove the first seven columns as they appear unrelated to predicting classe
train <- train_raw[, -c(1:7)]
# now let's identify columns with no missing values and get rid of the others
keepCols <- which(colMeans(is.na(train))==0)
train <- train[, keepCols]
```

Let's also look to see if we can use principal components analysis to reduce the number of columns we need:
```{r pca, echo=TRUE}
# how closely correlated are the predictors with each other?
cor(train[,1:52]) %>% melt(.) %>% filter(Var1!=Var2) %>% arrange(desc(value)) %>%
  ggplot() +
  geom_density(aes(value), fill="royalblue", alpha=.5, color="black") +
  theme_minimal() +
  xlab("Correlation") +
  ggtitle("Density Plot of Correlation Between Predictors")
  
# looks like some variables are highly correlated
# we can reduce the number of predictors with PCA
preProcValues <- preProcess(train, method = c("center", "scale","pca"))
traint <- predict(preProcValues, train)
testt <- predict(preProcValues, test)
```

## Data partitioning and model fitting

Here we partition the training data into a training set and a validation set. (We leave the test set for prediction later.)
```{r partition, echo=TRUE}
# now partition into a training and validation set
set.seed(007)
inTrain <- createDataPartition(traint$classe, p = 0.7, list = FALSE)
traint <- train[inTrain, ]
validt <- train[-inTrain, ]
```

We then fit three different ML models: (1) a random forest model, (2) a gradient boosting machine model, and (3) a linear discriminant analysis model. We will then review the results of each, focusing on the accuracy metric.
```{r models}
# fit 3 candidate models on the reduced data form of the training set
mod.rf <- train(classe ~ ., method = "rf", data=traint, trControl=trainControl(method='cv'), number=5, allowParallel=TRUE )
mod.gbm <- train(classe ~ ., method = "gbm", data=traint, verbose=FALSE)
mod.lda <- train(classe ~ ., method = "lda", data=traint)
```

Let's review the results of each model:
```{r review, echo=T}
mod.rf
mod.gbm
mod.lda
```

Let's compare results:
```{r results, echo=TRUE}
# check results
confusionMatrix(predict(mod.rf, traint), as.factor(traint$classe))
confusionMatrix(predict(mod.gbm, traint), as.factor(traint$classe))
confusionMatrix(predict(mod.lda, traint), as.factor(traint$classe))
# the random forest has by far the highest accuracy
```

And let's also confirm it works well on the validation set. We expect this out-of-sample error rate to be approximately 1 minus the accuracy of the chosen ML model (in this case, the random forest model with an in-sample accuracy of 98.8%), or approximately 1.2%.
```{r validation}
# apply to the validation set
validt.preds.rf <- predict(mod.rf, validt)
# still excellent accuracy
confusionMatrix(validt.preds.rf, as.factor(validt$classe))
```

## Prediction

Finally, let's apply once to the test set to get our final results:
```{r test}
# finally, apply to test set
test.preds.rf <- predict(mod.rf, test)

# view
test.preds.rf
```






